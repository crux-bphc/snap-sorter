{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"face_search_indian\" \n",
    "DIMENSION = 512\n",
    "MILVUS_HOST = \"localhost\"\n",
    "MILVUS_PORT = \"19530\"\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections\n",
    "\n",
    "connections.connect(host=MILVUS_HOST, port=MILVUS_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import utility\n",
    "\n",
    "if utility.has_collection(COLLECTION_NAME):\n",
    "    utility.drop_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, CollectionSchema, DataType, Collection\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(\n",
    "        name=\"filepath\", dtype=DataType.VARCHAR, max_length=1000\n",
    "    ),  \n",
    "    FieldSchema(name=\"image_embedding\", dtype=DataType.FLOAT_VECTOR, dim=DIMENSION),\n",
    "]\n",
    "schema = CollectionSchema(fields=fields)\n",
    "collection = Collection(name=COLLECTION_NAME, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = {\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"index_type\": \"FLAT\",\n",
    "}\n",
    "collection.create_index(field_name=\"image_embedding\", index_params=index_params)\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "paths = glob.glob(\"./archive/**/Priyanka_Chopra/*.jpg\", recursive=True)\n",
    "paths2 = glob.glob(\"./archive/**/Pooja_Hegde/*.jpg\", recursive=True)\n",
    "paths = paths + paths2\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def embed_search_split(ratio, paths):\n",
    "    # shuffle the paths\n",
    "    paths = np.random.permutation(paths)\n",
    "    split = int(len(paths) * ratio)\n",
    "    return paths[:split], paths[split:]\n",
    "\n",
    "train_paths, test_paths = embed_search_split(0.8, paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123, 103)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countPo = 0\n",
    "countPr = 0\n",
    "for path in train_paths:\n",
    "    if \"Pooj\" in path:\n",
    "        countPo+=1\n",
    "    else:\n",
    "        countPr+=1\n",
    "        \n",
    "countPo, countPr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "\n",
    "mtcnn = MTCNN(image_size=160, margin=0, min_face_size=20)\n",
    "\n",
    "def extract_face(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    img_cropped = mtcnn(img)\n",
    "    return img_cropped\n",
    "\n",
    "model = InceptionResnetV1(pretrained=\"vggface2\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "def embed(data):\n",
    "    with torch.no_grad():\n",
    "        output = model(torch.stack(data[0])).squeeze()\n",
    "        collection.insert([data[1], output.tolist()])\n",
    "\n",
    "\n",
    "data_batch = [[], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226, 57)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_paths), len(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countPo = 0\n",
    "countPr = 0\n",
    "for path in test_paths:\n",
    "    if \"Pooj\" in path:\n",
    "        countPo += 1\n",
    "    else:\n",
    "        countPr += 1\n",
    "\n",
    "countPo, countPr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226/226 [02:23<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "for path in tqdm(train_paths):\n",
    "    im = extract_face(path)\n",
    "    if im is None:\n",
    "        continue\n",
    "    data_batch[0].append(im)\n",
    "    data_batch[1].append(path)\n",
    "    if len(data_batch[0]) % BATCH_SIZE == 0:\n",
    "        embed(data_batch)\n",
    "        data_batch = [[], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(data):\n",
    "    with torch.no_grad():\n",
    "        ret = model(torch.stack(data))\n",
    "        # If more than one image, use squeeze\n",
    "        if len(ret) > 1:\n",
    "            return ret.squeeze().tolist()\n",
    "        # Squeeze would remove batch for single image, so using flatten\n",
    "        else:\n",
    "            return torch.flatten(ret, start_dim=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch = [[], []]\n",
    "\n",
    "for path in test_paths:\n",
    "    im = extract_face(path)\n",
    "    if im is None:\n",
    "        continue\n",
    "    data_batch[0].append(im)\n",
    "    data_batch[1].append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "embeds = embed(data_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "res = collection.search(\n",
    "    embeds,\n",
    "    anns_field=\"image_embedding\",\n",
    "    output_fields=[\"filepath\"],\n",
    "    param={\n",
    "        \"metric_type\": \"L2\",\n",
    "        \"params\": {\"radius\": 1.0},\n",
    "    },\n",
    "    limit=150,\n",
    ")\n",
    "finish = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 7\n",
    "len(res[N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res[N]\n",
    "rpaths = [x.entity.get(\"filepath\") for x in r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./archive/bollywood_celeb_faces_1/Priyanka_Chopra/79.jpg'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch[1][N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 51)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countPo = 0\n",
    "countPr = 0\n",
    "for path in rpaths:\n",
    "    if \"Pooj\" in path:\n",
    "        countPo += 1\n",
    "    else:\n",
    "        countPr += 1\n",
    "\n",
    "countPo, countPr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digiface-OH8A3OWD-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
